{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Crash Course\n",
    "## Bornhack 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.preprocessing import image as kp_image\n",
    "\n",
    "# Keras is only used to load VGG19 model as a high level API to TensorFlow \n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "# pillow is used for loading and saving images\n",
    "from PIL import Image\n",
    "\n",
    "# numPy is used for manipulation of array of object i.e Image in our case\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from style_transfer import load_img, deprocess_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config \n",
    "We need to define a couple of key parameters ahead of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input images for content and style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where the content and style images are located\n",
    "content_path = 'data/contents/content-eagle.jpg'\n",
    "style_path = 'data/styles/style-pattern-1.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to where Vgg19 model weight is located \n",
    "vgg_weights = \"data/vgg_weights/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output generated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result as\n",
    "save_name = 'generated-2.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  ML parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 200\n",
    "content_weight = 0.1\n",
    "style_weight = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of layers to be considered for calculation of Content and Style Loss\n",
    "content_layers = ['block3_conv3']\n",
    "style_layers = ['block1_conv1','block2_conv2','block4_conv3']\n",
    "\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Content Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_loss(content, target):\n",
    "    return tf.reduce_mean(tf.square(content - target)) /2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Style Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_loss(base_style, gram_target):\n",
    "\n",
    "    height, width, channels = base_style.get_shape().as_list()\n",
    "    gram_style = gram_matrix(base_style)\n",
    "\n",
    "    # Original eqn as a constant to divide i.e 1/(4. * (channels ** 2) * (width * height) ** 2)\n",
    "    return tf.reduce_mean(tf.square(gram_style - gram_target)) / (channels**2 * width * height) #(4.0 * (channels ** 2) * (width * height) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, loss_weights, generated_output_activations, \n",
    "                 gram_style_features, content_features, \n",
    "                 num_content_layers, num_style_layers):\n",
    "\n",
    "    generated_content_activations = generated_output_activations[:num_content_layers]\n",
    "    generated_style_activations   = generated_output_activations[num_content_layers:]\n",
    "\n",
    "    style_weight, content_weight = loss_weights\n",
    "\n",
    "    style_score = 0\n",
    "    content_score = 0\n",
    "\n",
    "    # Accumulate style losses from all layers\n",
    "    # Here, we equally weight each contribution of each loss layer\n",
    "    weight_per_style_layer = 1.0 / float(num_style_layers)\n",
    "    for target_style, comb_style in zip(gram_style_features, generated_style_activations):\n",
    "        temp = get_style_loss(comb_style[0], target_style)\n",
    "        style_score += weight_per_style_layer * temp\n",
    "\n",
    "    # Accumulate content losses from all layers \n",
    "    weight_per_content_layer = 1.0 / float(num_content_layers)\n",
    "    for target_content, comb_content in zip(content_features, generated_content_activations):\n",
    "        temp = get_content_loss(comb_content[0], target_content)\n",
    "        content_score += weight_per_content_layer* temp\n",
    "\n",
    "    # Get total loss\n",
    "    loss = style_weight*style_score + content_weight*content_score \n",
    "\n",
    "\n",
    "    return loss, style_score, content_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_representations(model, content_path, style_path, num_content_layers):\n",
    "    \"\"\"\n",
    "    Used to pass content and style image through the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load our images in \n",
    "    content_image = load_img(content_path)\n",
    "    style_image   = load_img(style_path)\n",
    "\n",
    "    # batch compute content and style features\n",
    "    content_outputs = model(content_image)\n",
    "    style_outputs   = model(style_image)\n",
    "\n",
    "    # Get the style and content feature representations from our model  \n",
    "    style_features   = [ style_layer[0]  for style_layer    in style_outputs[num_content_layers:] ]\n",
    "    content_features = [ content_layer[0] for content_layer in content_outputs[:num_content_layers] ]\n",
    "\n",
    "    return style_features, content_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gram Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "\n",
    "    # if input tensor is a 3D array of size Nh x Nw X Nc\n",
    "    # we reshape it to a 2D array of Nc x (Nh*Nw)\n",
    "    channels = int(input_tensor.shape[-1])\n",
    "    a = tf.reshape(input_tensor, [-1, channels])\n",
    "    n = tf.shape(a)[0]\n",
    "\n",
    "    # get gram matrix \n",
    "    gram = tf.matmul(a, a, transpose_a=True)\n",
    "\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Keras Load VGG19 model\n",
    "def get_model(content_layers,style_layers):\n",
    "\n",
    "    # Load our model. We load pretrained VGG, trained on imagenet data\n",
    "    vgg19 = VGG19(weights=None, include_top=False)\n",
    "\n",
    "    # We don't need to (or want to) train any layers of our pre-trained vgg model, so we set it's trainable to false.\n",
    "    vgg19.trainable = False\n",
    "\n",
    "    style_model_outputs = [vgg19.get_layer(name).output for name in style_layers]\n",
    "    content_model_outputs = [vgg19.get_layer(name).output for name in content_layers]\n",
    "\n",
    "    model_outputs = content_model_outputs + style_model_outputs\n",
    "\n",
    "    # Build model \n",
    "    return Model(inputs = vgg19.input, outputs = model_outputs),  vgg19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensorflow session \n",
    "sess = tf.Session()\n",
    "\n",
    "# Assign keras back-end to the TF session which we created\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Obtain the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0718 21:44:07.016608 4786664896 deprecation_wrapper.py:119] From /Users/mateuszjurewicz/Bornhack ML Crashcourse/bornhack_ml_crashcourse/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0718 21:44:07.018614 4786664896 deprecation_wrapper.py:119] From /Users/mateuszjurewicz/Bornhack ML Crashcourse/bornhack_ml_crashcourse/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0718 21:44:07.024144 4786664896 deprecation_wrapper.py:119] From /Users/mateuszjurewicz/Bornhack ML Crashcourse/bornhack_ml_crashcourse/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0718 21:44:07.077327 4786664896 deprecation_wrapper.py:119] From /Users/mateuszjurewicz/Bornhack ML Crashcourse/bornhack_ml_crashcourse/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, vgg19 = get_model(content_layers,style_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Get the style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the style and content feature representations (from our specified intermediate layers) \n",
    "style_features, content_features = get_feature_representations(model, content_path, style_path, num_content_layers)\n",
    "gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG default normalization\n",
    "norm_means = np.array([103.939, 116.779, 123.68])\n",
    "min_vals = -norm_means\n",
    "max_vals = 255 - norm_means "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF variable for generated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the original content image, as it will become the generated one\n",
    "generated_image = load_img(content_path)\n",
    "\n",
    "# create tensorflow variable to hold a stylized/generated image during the training \n",
    "generated_image = tf.Variable(generated_image, dtype=tf.float32)\n",
    "\n",
    "# pass it to the model\n",
    "model_outputs = model(generated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Losses & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weightages of each content and style images i.e alpha & beta\n",
    "loss_weights = (style_weight, content_weight)\n",
    "\n",
    "# Create our optimizer\n",
    "loss = compute_loss(model, loss_weights, model_outputs, gram_style_features, content_features, num_content_layers, num_style_layers)\n",
    "opt = tf.train.AdamOptimizer(learning_rate=9, beta1=0.9, epsilon=1e-1).minimize( loss[0], var_list = [generated_image])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize the TF variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(generated_image.initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the downloaded vgg19 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0718 21:44:17.475494 4786664896 deprecation_wrapper.py:119] From /Users/mateuszjurewicz/Bornhack ML Crashcourse/bornhack_ml_crashcourse/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0718 21:44:17.476459 4786664896 deprecation_wrapper.py:119] From /Users/mateuszjurewicz/Bornhack ML Crashcourse/bornhack_ml_crashcourse/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loading the weights again because tf.global_variables_initializer() resets the weights\n",
    "vgg19.load_weights(vgg_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put loss as infinity before training starts and Create a variable to hold best image (i.e image with minimum loss)\n",
    "best_loss, best_img = float('inf'), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best: iteration:  0 loss:  15295194000.0   style_loss:  16994658000.0   content_loss:  24112.22\n",
      "best: iteration:  1 loss:  12139307000.0   style_loss:  13488112000.0   content_loss:  67060.7\n",
      "best: iteration:  2 loss:  8771731000.0   style_loss:  9746355000.0   content_loss:  125043.42\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_iterations):\n",
    "\n",
    "    # Do optimization\n",
    "    sess.run(opt)\n",
    "\n",
    "    # Make sure image values stays in the range of max-min value of VGG norm \n",
    "    clipped = tf.clip_by_value(generated_image, min_vals, max_vals)\n",
    "    # assign the clipped value to the tensor stylized image\n",
    "    generated_image.assign(clipped)\n",
    "\n",
    "\n",
    "    # Open the Tuple of tensors \n",
    "    total_loss, style_score, content_score = loss\n",
    "    total_loss = total_loss.eval(session=sess)\n",
    "\n",
    "\n",
    "    if total_loss < best_loss:\n",
    "\n",
    "          # Update best loss and best image from total loss. \n",
    "          best_loss = total_loss\n",
    "\n",
    "          # generated image is of shape (1, h, w, 3) convert it to (h, w, 3)\n",
    "          temp_generated_image = sess.run(generated_image)[0]\n",
    "          best_img = deprocess_img(temp_generated_image)\n",
    "\n",
    "          s_loss = sess.run(style_score)\n",
    "          c_loss = sess.run(content_score)\n",
    "\n",
    "          # print best loss\n",
    "          print('best: iteration: ', i ,'loss: ', total_loss ,'  style_loss: ',  s_loss,'  content_loss: ', c_loss)\n",
    "\n",
    "    # Save image after every X iterations \n",
    "    if (i+1)%10 == 0:\n",
    "          output = Image.fromarray(best_img)\n",
    "          output.save(str(i+1)+'-'+save_name)\n",
    "\n",
    "# after num_iterations iterations are completed, close the TF session \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
