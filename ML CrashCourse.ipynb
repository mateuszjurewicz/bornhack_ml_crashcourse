{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Crash Course\n",
    "## Bornhack 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.preprocessing import image as kp_image\n",
    "\n",
    "# Keras is only used to load VGG19 model as a high level API to TensorFlow \n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "# pillow is used for loading and saving images\n",
    "from PIL import Image\n",
    "\n",
    "# numPy is used for manipulation of array of object i.e Image in our case\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of layers to be considered for calculation of Content and Style Loss\n",
    "content_layers = ['block3_conv3']\n",
    "style_layers   = ['block1_conv1','block2_conv2','block4_conv3']\n",
    "\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers   = len(style_layers)\n",
    "\n",
    "# path where the content and style images are located\n",
    "content_path = 'data/contents/content-eagle.jpg'\n",
    "style_path   = 'data/styles/style-pattern-1.jpg'\n",
    "\n",
    "# save the result as\n",
    "save_name = 'generated.jpg'\n",
    "\n",
    "# path to where Vgg19 model weight is located \n",
    "vgg_weights = \"data/vgg_weights/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Content Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_loss(content, target):\n",
    "    return tf.reduce_mean(tf.square(content - target)) /2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Style Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_loss(base_style, gram_target):\n",
    "\n",
    "    height, width, channels = base_style.get_shape().as_list()\n",
    "    gram_style = gram_matrix(base_style)\n",
    "\n",
    "    # Original eqn as a constant to divide i.e 1/(4. * (channels ** 2) * (width * height) ** 2)\n",
    "    return tf.reduce_mean(tf.square(gram_style - gram_target)) / (channels**2 * width * height) #(4.0 * (channels ** 2) * (width * height) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, loss_weights, generated_output_activations, \n",
    "                 gram_style_features, content_features, \n",
    "                 num_content_layers, num_style_layers):\n",
    "\n",
    "    generated_content_activations = generated_output_activations[:num_content_layers]\n",
    "    generated_style_activations   = generated_output_activations[num_content_layers:]\n",
    "\n",
    "    style_weight, content_weight = loss_weights\n",
    "\n",
    "    style_score = 0\n",
    "    content_score = 0\n",
    "\n",
    "    # Accumulate style losses from all layers\n",
    "    # Here, we equally weight each contribution of each loss layer\n",
    "    weight_per_style_layer = 1.0 / float(num_style_layers)\n",
    "    for target_style, comb_style in zip(gram_style_features, generated_style_activations):\n",
    "        temp = get_style_loss(comb_style[0], target_style)\n",
    "        style_score += weight_per_style_layer * temp\n",
    "\n",
    "    # Accumulate content losses from all layers \n",
    "    weight_per_content_layer = 1.0 / float(num_content_layers)\n",
    "    for target_content, comb_content in zip(content_features, generated_content_activations):\n",
    "        temp = get_content_loss(comb_content[0], target_content)\n",
    "        content_score += weight_per_content_layer* temp\n",
    "\n",
    "    # Get total loss\n",
    "    loss = style_weight*style_score + content_weight*content_score \n",
    "\n",
    "\n",
    "    return loss, style_score, content_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_representations(model, content_path, style_path, num_content_layers):\n",
    "    \"\"\"\n",
    "    Used to pass content and style image through the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load our images in \n",
    "    content_image = load_img(content_path)\n",
    "    style_image   = load_img(style_path)\n",
    "\n",
    "    # batch compute content and style features\n",
    "    content_outputs = model(content_image)\n",
    "    style_outputs   = model(style_image)\n",
    "\n",
    "    # Get the style and content feature representations from our model  \n",
    "    style_features   = [ style_layer[0]  for style_layer    in style_outputs[num_content_layers:] ]\n",
    "    content_features = [ content_layer[0] for content_layer in content_outputs[:num_content_layers] ]\n",
    "\n",
    "    return style_features, content_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gram Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "\n",
    "    # if input tensor is a 3D array of size Nh x Nw X Nc\n",
    "    # we reshape it to a 2D array of Nc x (Nh*Nw)\n",
    "    channels = int(input_tensor.shape[-1])\n",
    "    a = tf.reshape(input_tensor, [-1, channels])\n",
    "    n = tf.shape(a)[0]\n",
    "\n",
    "    # get gram matrix \n",
    "    gram = tf.matmul(a, a, transpose_a=True)\n",
    "\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Keras Load VGG19 model\n",
    "def get_model(content_layers,style_layers):\n",
    "\n",
    "    # Load our model. We load pretrained VGG, trained on imagenet data\n",
    "    vgg19 = VGG19(weights=None, include_top=False)\n",
    "\n",
    "    # We don't need to (or want to) train any layers of our pre-trained vgg model, so we set it's trainable to false.\n",
    "    vgg19.trainable = False\n",
    "\n",
    "    style_model_outputs = [vgg19.get_layer(name).output for name in style_layers]\n",
    "    content_model_outputs = [vgg19.get_layer(name).output for name in content_layers]\n",
    "\n",
    "    model_outputs = content_model_outputs + style_model_outputs\n",
    "\n",
    "    # Build model \n",
    "    return Model(inputs = vgg19.input, outputs = model_outputs),  vgg19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
