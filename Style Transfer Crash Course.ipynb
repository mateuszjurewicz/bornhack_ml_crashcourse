{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer Crash Course\n",
    "## Bornhack 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.preprocessing import image as kp_image\n",
    "\n",
    "# Keras is only used to load VGG19 model as a high level API to TensorFlow \n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "# image functions\n",
    "from PIL import Image\n",
    "from IPython.core.display import HTML, display, Image as DisplayImage\n",
    "\n",
    "# numPy is used for manipulation of array of object i.e Image in our case\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from style_transfer import load_img, deprocess_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confirm virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_side_to_side_two(path1, path2, width=300, height=400):\n",
    "    display(HTML(\"<table><tr><td><img src='{0}' width={2} height={3}></td><td><img src='{1}' width={2} height={3}></td></tr></table>\".format(path1, path2, width, height)))\n",
    "    \n",
    "def display_side_to_side_three(path1, path2, path3, width=300, height=400):\n",
    "    display(HTML(\"<table><tr><td><img src='{0}' width={3} height={4}></td><td><img src='{1}' width={3} height={4}></td><td><img src='{2}' width={3} height={4}></td></tr></table>\".format(path1, path2, path3, width, height)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config \n",
    "We need to define a couple of key parameters ahead of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input images for content and style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where the content and style images are located\n",
    "content_path = 'data/contents/content-horseduck.jpg'\n",
    "style_path = 'data/styles/style-dali-2.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_side_to_side_two(content_path, style_path, width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to where Vgg19 model weight is located \n",
    "vgg_weights = \"data/vgg_weights/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output generated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result as\n",
    "save_name = 'generated.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  ML parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 200\n",
    "content_weight = 0.1\n",
    "style_weight = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of layers to be considered for calculation of Content and Style Loss\n",
    "content_layers = ['block3_conv3']\n",
    "style_layers = ['block1_conv1','block2_conv2','block4_conv3']\n",
    "\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_representations(model, content_path, style_path, num_content_layers):\n",
    "    \"\"\"\n",
    "    Used to pass content and style image through the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load our images in \n",
    "    content_image = load_img(content_path)\n",
    "    style_image   = load_img(style_path)\n",
    "\n",
    "    # batch compute content and style features\n",
    "    content_outputs = model(content_image)\n",
    "    style_outputs   = model(style_image)\n",
    "\n",
    "    # Get the style and content feature representations from our model  \n",
    "    style_features   = [ style_layer[0]  for style_layer    in style_outputs[num_content_layers:] ]\n",
    "    content_features = [ content_layer[0] for content_layer in content_outputs[:num_content_layers] ]\n",
    "\n",
    "    return style_features, content_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Content Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_loss(content, target):\n",
    "    return tf.reduce_mean(tf.square(content - target)) /2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Style Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_loss(base_style, gram_target):\n",
    "\n",
    "    height, width, channels = base_style.get_shape().as_list()\n",
    "    gram_style = gram_matrix(base_style)\n",
    "\n",
    "    # Original eqn as a constant to divide i.e 1/(4. * (channels ** 2) * (width * height) ** 2)\n",
    "    return tf.reduce_mean(tf.square(gram_style - gram_target)) / (channels**2 * width * height) #(4.0 * (channels ** 2) * (width * height) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gram Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "\n",
    "    # if input tensor is a 3D array of size Nh x Nw X Nc\n",
    "    # we reshape it to a 2D array of Nc x (Nh*Nw)\n",
    "    channels = int(input_tensor.shape[-1])\n",
    "    a = tf.reshape(input_tensor, [-1, channels])\n",
    "    n = tf.shape(a)[0]\n",
    "\n",
    "    # get gram matrix \n",
    "    gram = tf.matmul(a, a, transpose_a=True)\n",
    "\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, loss_weights, generated_output_activations, \n",
    "                 gram_style_features, content_features, \n",
    "                 num_content_layers, num_style_layers):\n",
    "\n",
    "    generated_content_activations = generated_output_activations[:num_content_layers]\n",
    "    generated_style_activations   = generated_output_activations[num_content_layers:]\n",
    "\n",
    "    style_weight, content_weight = loss_weights\n",
    "\n",
    "    style_score = 0\n",
    "    content_score = 0\n",
    "\n",
    "    # Accumulate style losses from all layers\n",
    "    # Here, we equally weight each contribution of each loss layer\n",
    "    weight_per_style_layer = 1.0 / float(num_style_layers)\n",
    "    for target_style, comb_style in zip(gram_style_features, generated_style_activations):\n",
    "        temp = get_style_loss(comb_style[0], target_style)\n",
    "        style_score += weight_per_style_layer * temp\n",
    "\n",
    "    # Accumulate content losses from all layers \n",
    "    weight_per_content_layer = 1.0 / float(num_content_layers)\n",
    "    for target_content, comb_content in zip(content_features, generated_content_activations):\n",
    "        temp = get_content_loss(comb_content[0], target_content)\n",
    "        content_score += weight_per_content_layer* temp\n",
    "\n",
    "    # Get total loss\n",
    "    loss = style_weight*style_score + content_weight*content_score \n",
    "\n",
    "\n",
    "    return loss, style_score, content_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Keras Load VGG19 model\n",
    "def get_model(content_layers,style_layers):\n",
    "\n",
    "    # Load our model. We load pretrained VGG, trained on imagenet data\n",
    "    vgg19 = VGG19(weights=None, include_top=False)\n",
    "\n",
    "    # We don't need to (or want to) train any layers of our pre-trained vgg model, so we set it's trainable to false.\n",
    "    vgg19.trainable = False\n",
    "\n",
    "    style_model_outputs = [vgg19.get_layer(name).output for name in style_layers]\n",
    "    content_model_outputs = [vgg19.get_layer(name).output for name in content_layers]\n",
    "\n",
    "    model_outputs = content_model_outputs + style_model_outputs\n",
    "\n",
    "    # Build model \n",
    "    return Model(inputs = vgg19.input, outputs = model_outputs),  vgg19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensorflow session \n",
    "sess = tf.Session()\n",
    "\n",
    "# Assign keras back-end to the TF session which we created\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Obtain the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vgg19 = get_model(content_layers,style_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Get the style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the style and content feature representations (from our specified intermediate layers) \n",
    "style_features, content_features = get_feature_representations(model, content_path, style_path, num_content_layers)\n",
    "gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG default normalization\n",
    "norm_means = np.array([103.939, 116.779, 123.68])\n",
    "min_vals = -norm_means\n",
    "max_vals = 255 - norm_means "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF variable for generated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the original content image, as it will become the generated one\n",
    "generated_image = load_img(content_path)\n",
    "\n",
    "# create tensorflow variable to hold a stylized/generated image during the training \n",
    "generated_image = tf.Variable(generated_image, dtype=tf.float32)\n",
    "\n",
    "# pass it to the model\n",
    "model_outputs = model(generated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Losses & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weightages of each content and style images i.e alpha & beta\n",
    "loss_weights = (style_weight, content_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute loss\n",
    "loss = compute_loss(model, loss_weights, model_outputs, gram_style_features, content_features, num_content_layers, num_style_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate an optimizer \n",
    "opt = tf.train.AdamOptimizer(learning_rate=9, beta1=0.9, epsilon=1e-1).minimize(loss[0], var_list = [generated_image])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize the TF variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(generated_image.initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the downloaded vgg19 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the weights again because tf.global_variables_initializer() resets the weights\n",
    "vgg19.load_weights(vgg_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put loss as infinity before training starts and Create a variable to hold best image (i.e image with minimum loss)\n",
    "best_loss, best_img = float('inf'), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_iterations):\n",
    "\n",
    "    # Do optimization\n",
    "    sess.run(opt)\n",
    "\n",
    "    # Make sure image values stays in the range of max-min value of VGG norm \n",
    "    clipped = tf.clip_by_value(generated_image, min_vals, max_vals)\n",
    "    # assign the clipped value to the tensor stylized image\n",
    "    generated_image.assign(clipped)\n",
    "\n",
    "\n",
    "    # Open the Tuple of tensors \n",
    "    total_loss, style_score, content_score = loss\n",
    "    total_loss = total_loss.eval(session=sess)\n",
    "\n",
    "\n",
    "    if total_loss < best_loss:\n",
    "\n",
    "        # Update best loss and best image from total loss. \n",
    "        best_loss = total_loss\n",
    "\n",
    "        # generated image is of shape (1, h, w, 3) convert it to (h, w, 3)\n",
    "        temp_generated_image = sess.run(generated_image)[0]\n",
    "        best_img = deprocess_img(temp_generated_image)\n",
    "\n",
    "        s_loss = sess.run(style_score)\n",
    "        c_loss = sess.run(content_score)\n",
    "\n",
    "        # print best loss\n",
    "        print('best: iteration: ', i ,'loss: ', total_loss ,'  style_loss: ',  s_loss,'  content_loss: ', c_loss)\n",
    "\n",
    "    # Save image after every X iterations \n",
    "    if (i+1)%10 == 0:\n",
    "        output = Image.fromarray(best_img)\n",
    "        output.save(str(i+1)+'-'+save_name)\n",
    "        \n",
    "        # show\n",
    "        display_side_to_side_three(content_path, style_path, str(i+1)+'-'+save_name)\n",
    "\n",
    "# after num_iterations iterations are completed, close the TF session \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspection\n",
    "\n",
    "Show us the final images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(DisplayImage(content_path, width=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(DisplayImage(style_path, width=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(DisplayImage(str(100)+'-'+save_name, width=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
